请你分析D:\sqw\code\verilogtest\riscv\riscv.srcs下的代码，先搞清它的架构和功能

好的，现在我们把注意力转向dcache，对于mem_stage来说也有确定的接口，我们不改动它。我们关注点在dache上，后面连接到wishbone，wishbone也是现成的成熟接口我们不动它。请先分析dcache架构和实现

现在运行有些问题需要测试。不仅仅是L1 cache,包括L2也要测试。是在运行D:\sqw\code\verilogtest\riscv\test\test.s时发现出错的。你分析一下这个程序有什么特点，然后模拟他来进行读写测试。

请阅读memstage.sv,dcache.sv,dcache_l1.sv,dcache_l2.sv,搞清楚Dcache运作的方式

请在mem_stage.sv中加个mux把bypass的流量旁路直连wishbone，让它无需进入dcache处理。

现在这个实现在mem_stage.sv中是无法连续每个时钟周期输出一个字符吧？
既然往magic address写是可以连续操作的，将来如果实现数据输出输入缓存的话也需要连续单周期写和读，那么我们为什么不先把mem_stage.sv改成支持单周期连续操作的呢？请先通读各个文件搞清流水线CPU的工作原理，然后对mem_stage.sv进行修改让它支持连续单周期的输出。

还要考虑一点是：只有0x80000000~0x807fffff的访问是将来要用cache的，其他地址，例如uart的地址0x10000000是不经过dcache的。因此在mem_stage需要做判断：如果是magic adress立即ack，如果是uart adress 就bypass dache进入wishbone, 只有0x80000000~0x807fffff才进入dcache处理， 如果HIT的话立即返回，没有hit再继续处理。mem_stage的这个判断工作也可以放在dcache来做，如何处理比较好请你考虑。

我先进行git restore. 
然后我们重新考虑过：
1、mem_stage通过组合逻辑把地址/数据/读写等信号传给dcache，dcache用组合逻辑进行分类判断，对于能够即时返回的类型返回信号给mem_stage，这个mem_stage就可以保持IDLE无需进入WAIT状态，同时dcache进行处理：一、magic address类  二、cache hit类。对于不能即时返回结果的发信号让mem_stage进行wait，分两类，一、cachable的地址，进入后续的dcache处理，二、bypass的直接交给wishbone。
2、可以把wb_mux中对magicadress的处理去除，恢复为上一个版本的状态。这处理应该在dcache中进行。
3、只有0x80000000~0x807fffff的访问是将来要用cache的，其他地址，例如uart的地址0x10000000是不经过dcache的。

In MEM_IDLE: Check if cpu_mem_ack is already high (single-cycle operation)
If ACK is immediate, stay in IDLE (should not be  stalled)
If ACK is not immediate, enter MEM_WAIT_ACK as before
Only request stall if operation is not yet complete
Single-cycle operations should not stall

compile cmd:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v
run cmd:
vvp -n sim.vvp


不用考虑和模仿icache的实现方式。
我希望实现的dcache是：
L1 (LUTRAM): 1KB, Direct-Mapped, 单周期命中
L2 (BRAM): 32KB, 4-Way Set-Associative, 1周期延迟
写策略: Write-Back

编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v >NUL 2>&1
运行仿真命令用:
vvp -n sim.vvp

在D:\sqw\code\verilogtest\riscv\riscv.srcs\test下编写测试程序，编译成bin文件，复制到D:\sqw\code\verilogtest\riscv\riscv.srcs下，名为kernel.bin即可运行。

可以用magic address来输出，那样比较快，地址是0x90000000
程序运行不用等太长时间，最多等5秒

1、PTW will share the DCache Wishbone interface rather than having a dedicated memory port. When MMU is doing PTW, MEM stage is unconditionally stalled; MMU PTW completes before MEM stage resumes.

2、以下地址不可缓存，无需翻译，需要直通：
0x1000_0000 - 0x1000_FFFF	UART (64KB)
0x0200_0000 - 0x020F_FFFF	CLINT (1MB)
0x9000_0000 - 0x9FFF_FFFF	Magic输出 
3、计划清晰地划分成多个可验证测试的阶段，然后分阶段实施，并且逐个阶段进行详尽的测试，保证稳步推进

双MMU方案：
[IF Stage] --> [IMMU] --> wb_inst_* --> [Wishbone Master] --> ...
[MEM Stage] --> [DMMU] --> wb_data_* --> [Wishbone Master] --> ...


需要注意：
1、ptw是通过dcache读写内存的，目的是为了利用缓存功能。
2、以下地址不可缓存，无需翻译，需要直通：
0x1000_0000 - 0x1000_FFFF	UART (64KB)
0x0200_0000 - 0x020F_FFFF	CLINT (1MB)
0x9000_0000 - 0x9FFF_FFFF	Magic输出 
3、将来ptw会是来自DMMU和IMMU的ptw的arbitor.
4、计划清晰地划分成多个可验证测试的阶段，然后分阶段实施，并且逐个阶段进行详尽的测试，保证稳步推进
5、在D:\sqw\code\verilogtest\riscv\riscv.srcs\test下编写测试程序，编译成bin文件，复制到D:\sqw\code\verilogtest\riscv\riscv.srcs下，名为kernel.bin即可运行。

可以用magic address来输出，那样比较快，地址是0x90000000
程序运行不用等太长时间，最多等5秒

riscv64-unknown-elf-gcc -march=rv32i -mabi=ilp32 -c start.s -o start.o && riscv64-unknown-elf-gcc -DSIMU -march=rv32i -mabi=ilp32 -O2 -c dmmu_ptw_test.c -o dmmu_ptw_test.o && riscv64-unknown-elf-gcc -march=rv32i -mabi=ilp32 -nostdlib -T link.ld -o dmmu_ptw_test.elf start.o dmmu_ptw_test.o && riscv64-unknown-elf-objcopy -O binary dmmu_ptw_test.elf dmmu_ptw_test.bin && cp dmmu_ptw_test.bin ../kernel.bin

编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v >NUL 2>&1
运行仿真命令用:
vvp -n sim.vvp

我计划让mmu进行ptw时是通过dcache去访问sram内存，这样可以用上缓存功能。这就要求dcache不仅接入mem_stage，还要接入ptw。将来会有两个mmu，一个是用于指令的IMMU，一个是用于数据的DMMU，也就有两个ptw，怎样接入dcache?帮我设计一下

1、新增 ptw_arbiter.sv：仲裁 IMMU 和 DMMU 的 PTW 请求
2、修改 dcache.sv：实现真正的 PTW 访问逻辑
3、先进行各项测试再真正接入IF stage和mem stage，测试方法类似于mmu_tb.sv
4、编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v >NUL 2>&1
运行仿真命令用:
vvp  sim.vvp

我计划让mmu进行ptw时是通过dcache去访问sram内存，这样可以用上缓存功能。这就要求dcache不仅接入mem_stage，还要接入ptw。将来会有两个mmu，一个是用于指令的IMMU，一个是用于数据的DMMU，也就有两个ptw，通过arbitor接入dcache。

现在新增 ptw_arbiter.sv和修改dcache.sv的工作已经完成，我需要做各项完整的测试再真正接入IF stage和mem stage。
测试方法类似于mmu_tb.sv。
请你创建详尽的测试文件，在其中实例化出immu和dmmu，让它们真正进行ptw访问内存。
预先在内存上建立页表，可以采用 testbench.sv 中的 write_sram_word 和 read_sram_word 的方法。

编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v >NUL 2>&1
运行仿真命令用:
vvp  sim.vvp

我要测试的内容包括cache功能，不能bypass.
请你修改测试程序，SRAM的地址范围是：0x80000000~0x807fffff。包括baseram:0x80000000~0x803fffff和extram：0x80400000~0x807fffff。先建立起这两块RAM。然后写对应的write_sram_word和read_sram_word。再进行页表的配置和测试。同时也要测试通过dcache进行数据读写的通道(mem_stage读写dcache的方式)是否正常。
建立SRAM可以照搬testbench.sv中的相关代码，
对sram进行直接读写也可以参考testbench.sv中的相关代码，这可以用于方便地设置页表。
测试模块的名字换成testmmu.sv吧，把ptw_dcache_tb.sv改名为testmmu.sv
编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testmmu *.sv *.v 
运行仿真命令用:
vvp -n sim.vvp

问题现象：L2 All-Way Dirty Eviction 测试导致仿真死循环/超时。 根本原因： 当 L1 驱逐导致 L2 冲突（需要 Writeback L2 牺牲行）时，状态机流程错误：

错误流程：L1_TO_L2 (发现冲突) -> L2_WRITEBACK (写回牺牲行) -> L2_REFILL (错误！)
后果：应该返回 L1_TO_L2 写入刚才被驱逐的 L1 数据，却错误地跳转到 L2_REFILL 去从内存重新读取（读到了旧数据或垃圾数据），并且丢失了原本要写入 L2 的 L1 dirty 数据。这不仅导致数据丢失，还可能引发无限状态循环。
修复方案： 引入了 l2_evict_from_l1 寄存器来追踪 Writeback 的来源：

如果来自 L1 Eviction，L2_WRITEBACK 完成后返回 L1_TO_L2（并清除 dirty 位），确保数据正确写入 L2。
如果来自 L2 Miss，保持原逻辑跳转 L2_REFILL。

D:\sqw\code\verilogtest\riscv\riscv.srcs 这是一个五级流水线CPU项目。现在需要实现页表和虚拟内存。已经有mmu通过ptw_arbiter接入dcache来进行地址翻译。在testmmu.sv中有测试代码，进行了各项测试。现在要实现把取指和访存功能分别通过IMMU和DMMU实现虚拟内存功能。请你设计方案，把步骤划分得细一些，每一步有详尽的测试方案。逐步推进。
在D:\sqw\code\verilogtest\riscv\riscv.srcs\test下编写测试程序，编译成bin文件，复制到D:\sqw\code\verilogtest\riscv\riscv.srcs下，名为kernel.bin即可运行。
在D:\sqw\code\verilogtest\riscv\riscv.srcs\kernel 下有将来需要真正运行的kernel.bin
编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v 
运行仿真命令用:
vvp -n sim.vvp

D:\sqw\code\verilogtest\riscv\riscv.srcs\ucore 请分析该目录
编译：
rm -rf obj && make  
riscv64-unknown-elf-objdump -S bin/kernel > bin/kernel.asm
QEMU仿真：
qemu-system-riscv32 -M virt -bios .\bin\rbl.img -device loader,addr=0x80400000,file=.\bin\ucore.img -nographic

D:\sqw\code\verilogtest\riscv\riscv.srcs是一个CPU的verilog实现。
D:\sqw\code\verilogtest\riscv\riscv.srcs\ucore是这个CPU运行的软件。
rbl.img加载在0x80000000,ucore.img加载在0x80400000。
rbl的源码在"D:\sqw\code\verilogtest\riscv\riscv.srcs\rbl"，这个只能看不能改。
ucore的源码在"D:\sqw\code\verilogtest\riscv\riscv.srcs\ucore"，这个可以看可以改也可以加调试信息，改以后交给我生成img文件上板测试，测试结果再返回给你。
启动可以完成，出现user sh is running!!!$ 
这时可以输入"ls"命令。
现在的问题是输完命令回车以后系统就不再有响应了。看波形图指令定在0x800007b0不动，指令码是：07f58593
请从主要从软件入手来进行调试，确认是硬件某个地方问题再改硬件。因为在FPEG上运行速度比较快，但仿真和综合都很费时。你可以在软件中加调试信息，生成img文件以后给我加载到板上运行再回馈给你运行结果。


经测试并未生效。
我希望如此操作：
先回滚所做的改动，创建一个测试方法，看所说的问题是否存在，然后再做改动，看所说的问题是否消除。这样既可以确定该问题是否存在也可以确定改动是否生效。
在D:\sqw\code\verilogtest\riscv\riscv.srcs\test下编写测试程序，编译成bin文件，复制到D:\sqw\code\verilogtest\riscv\riscv.srcs下，名为kernel.bin即可运行。
编译命令用:
iverilog -g 2012 -D SIMU -o sim.vvp -s testbench *.sv *.v 
运行仿真命令用:
vvp -n sim.vvp